{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66d39512",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd3ad8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/panos/dev/hf_seg/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import JaccardIndex\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from hydra import initialize, compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808536e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Add the project root directory to the Python path\n",
    "cur_dir     = Path.cwd()\n",
    "project_dir = cur_dir.parent\n",
    "sys.path.append(str(project_dir))\n",
    "\n",
    "from models.teacher import Teacher\n",
    "from models.student import Student\n",
    "from models.tools import CombinedLoss\n",
    "from src.validate import evaluate_model\n",
    "from src.train import train_and_validate\n",
    "from data.kitti360.dataset import KittiSemSegDataset\n",
    "from utils.others import save_checkpoint, load_checkpoint, get_memory_footprint\n",
    "from data.kitti360.labels_kitti360 import trainId2label, NUM_CLASSES\n",
    "from utils.visualization import plot_image_and_masks\n",
    "from utils.others import save_checkpoint, load_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d078212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eb4c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(version_base=None, config_path=f\"../configs\", job_name=\"train_and_log\"):\n",
    "    cfg = compose(config_name=\"distil_config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c81163",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b0e3507",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = (cfg.augmentation.crop_height, cfg.augmentation.crop_width)\n",
    "train_transform = A.Compose([\n",
    "    # -- Geometric --\n",
    "    A.RandomCrop(height=crop_size[0], width=crop_size[1], p=1.0), # preserve scale/context\n",
    "    A.Affine(\n",
    "        # translate_percent={\"x\": (-0.05, 0.05), \"y\": (-0.05, 0.05)}, # ±5% shift\n",
    "        scale=(0.8, 1.0),                                           # zoom between 0.8×–1.0×\n",
    "        rotate=(-3, 3),                                             # ±3° roll\n",
    "        interpolation=cv2.INTER_LINEAR,\n",
    "        mask_interpolation=cv2.INTER_NEAREST,\n",
    "        border_mode=cv2.BORDER_CONSTANT,\n",
    "        fill=255,\n",
    "        fill_mask=255,\n",
    "        p=0.7\n",
    "    ),\n",
    "    A.Perspective(scale=(0.01, 0.03), p=0.5),  # tiny camera viewpoint warp\n",
    "\n",
    "    # -- Photometric --\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.5),\n",
    "    A.RandomGamma(gamma_limit=(90, 110), p=0.5),\n",
    "    A.OneOf([\n",
    "        A.RandomFog(fog_coef_range=(0.05, 0.2), p=1.0),\n",
    "        A.RandomShadow(shadow_roi=(0, 0.5, 1, 1), num_shadows_limit=(1, 2), p=1.0),\n",
    "        A.RandomSunFlare(src_radius=50, p=1.0)\n",
    "    ], p=0.5),\n",
    "\n",
    "    # -- Occlusions --\n",
    "    A.CoarseDropout(num_holes_range=(1, 4), \n",
    "                    hole_height_range=(5, 30), \n",
    "                    hole_width_range=(5, 30), \n",
    "                    p=0.5),                                # random occlusion\n",
    "\n",
    "    # — Blur & noise: motion, sensor, compression —\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(blur_limit=5, p=0.4),\n",
    "        A.GaussianBlur(blur_limit=(3,5), p=0.3),\n",
    "        A.MedianBlur(blur_limit=3, p=0.2),\n",
    "    ], p=0.5),\n",
    "    A.GaussNoise(\n",
    "        std_range=(10.0/255.0, 50.0/255.0),\n",
    "        mean_range=(0.0, 0.0),\n",
    "        p=0.5\n",
    "    )\n",
    "])\n",
    "\n",
    "# Define deterministic transforms for validation\n",
    "val_transform = A.Compose([\n",
    "    A.CenterCrop(height=crop_size[0], width=crop_size[1])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ae8aa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 7042\n",
      "Validation dataset size: 783\n"
     ]
    }
   ],
   "source": [
    "# Dataset and DataLoader\n",
    "dataset_root = '/home/panos/Documents/data/kitti-360'\n",
    "train_dataset = KittiSemSegDataset(dataset_root, train=True, transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.train.batch_size, \n",
    "                            shuffle=True, num_workers=cfg.dataset.num_workers, pin_memory=True)\n",
    "val_dataset = KittiSemSegDataset(dataset_root, train=False, transform=val_transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=cfg.train.batch_size,\n",
    "                        shuffle=False, num_workers=cfg.dataset.num_workers, pin_memory=True)\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ea0bb1",
   "metadata": {},
   "source": [
    "# Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2e8f90",
   "metadata": {},
   "source": [
    "Start simple: shrink backbone depth by half, reduce FPN channels by half, distill only with feature-mimic on the FPN’s fused map.\n",
    "\n",
    "Add attention transfer: hook into the student’s Transformer encoder to pull out its attention weights and compute AT loss against the teacher’s.\n",
    "\n",
    "Iterate: measure how each component (fewer layers vs. narrower vs. attention loss) affects final mIoU or accuracy, and re-balance your loss weights accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f637acb8",
   "metadata": {},
   "source": [
    "# Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bddb8553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Teacher(\n",
       "  (backbone): Dinov2Model(\n",
       "    (embeddings): Dinov2Embeddings(\n",
       "      (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Dinov2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x Dinov2Layer(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attention): Dinov2Attention(\n",
       "            (attention): Dinov2SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Dinov2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (layer_scale1): Dinov2LayerScale()\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Dinov2MLP(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_scale2): Dinov2LayerScale()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (head): FPNHead(\n",
       "    (projs): ModuleList(\n",
       "      (0-3): 4 x Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Dropout2d(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fuse): Sequential(\n",
       "      (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout2d(p=0.1, inplace=False)\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout2d(p=0.1, inplace=False)\n",
       "      (4): Conv2d(256, 33, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher = Teacher(\n",
    "    cfg.dataset.num_classes, \n",
    "    cfg.model.teacher\n",
    ")\n",
    "teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad264673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Memory Footprint ===\n",
      "Backbone: 86,580,480 params, 330.28 MB\n",
      "Head:     3,747,617 params, 14.30 MB\n",
      "Total:    90,328,097 params, 344.57 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(90328097, 361312388)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_memory_footprint(teacher, detailed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26316563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FP32 weights from /home/panos/dev/hf_seg/checkpoints/dino-fpn-bn.pth into teacher.\n"
     ]
    }
   ],
   "source": [
    "# Load the teacher model\n",
    "ckpt_path = project_dir / \"checkpoints\" / \"dino-fpn-bn.pth\"\n",
    "if not ckpt_path.exists():\n",
    "    print(f\"[Error] Checkpoint not found: {ckpt_path}\")\n",
    "checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "teacher.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "print(f\"Loaded FP32 weights from {ckpt_path} into teacher.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a5a695",
   "metadata": {},
   "source": [
    "# Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Student(\n",
       "  (backbone): Dinov2Model(\n",
       "    (embeddings): Dinov2Embeddings(\n",
       "      (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Dinov2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-2): 3 x Dinov2Layer(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attention): Dinov2Attention(\n",
       "            (attention): Dinov2SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Dinov2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (layer_scale1): Dinov2LayerScale()\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Dinov2MLP(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_scale2): Dinov2LayerScale()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (head): FPNHead(\n",
       "    (projs): ModuleList(\n",
       "      (0-3): 4 x Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Dropout2d(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fuse): Sequential(\n",
       "      (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout2d(p=0.1, inplace=False)\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout2d(p=0.1, inplace=False)\n",
       "      (4): Conv2d(256, 33, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student = Student(\n",
    "    cfg.dataset.num_classes, \n",
    "    cfg.model.student\n",
    ")\n",
    "student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcb1260c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Memory Footprint ===\n",
      "Backbone: 22,775,808 params, 86.88 MB\n",
      "Head:     3,747,617 params, 14.30 MB\n",
      "Total:    26,523,425 params, 101.18 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(26523425, 106093700)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_memory_footprint(student, detailed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf1e8ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5721cea3",
   "metadata": {},
   "source": [
    "# Loss and Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05db52b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "optimizer = optim.Adam(student.parameters(), lr=cfg.train.learning_rate)\n",
    "miou_metric = JaccardIndex(\n",
    "    task='multiclass',\n",
    "    num_classes=cfg.dataset.num_classes,\n",
    "    average='micro',\n",
    "    ignore_index=255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4124fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hyper-parameters for distillation\n",
    "# alpha = cfg.distill.alpha       # weight for CE loss\n",
    "# beta  = cfg.distill.beta        # weight for feature‐mimic loss\n",
    "# gamma = cfg.distill.gamma       # weight for logits‐KD (optional)\n",
    "# T     = cfg.distill.temperature # temperature for logits‐KD\n",
    "\n",
    "# def distillation_loss(student_logits, student_feats,\n",
    "#                       teacher_logits, teacher_feats,\n",
    "#                       targets):\n",
    "#     # 1) CE on full-res student logits\n",
    "#     ce = F.cross_entropy(student_logits, targets, ignore_index=255)\n",
    "\n",
    "#     # 2) Feature-based distillation (MSE):\n",
    "#     fm = F.mse_loss(student_feats, teacher_feats)\n",
    "\n",
    "#     # 3) (optional) logits-based distillation on low-res maps\n",
    "#     #    reshape [B, C, h, w] → [B, C, h*w], then apply KL\n",
    "#     if gamma > 0:\n",
    "#         B, C, h, w = student_logits.shape\n",
    "#         s = student_logits.view(B, C, -1).permute(0,2,1)\n",
    "#         t = teacher_logits .view(B, C, -1).permute(0,2,1)\n",
    "\n",
    "#         log_p_s = F.log_softmax(s / T, dim=2)\n",
    "#         p_t     = F.softmax    (t / T, dim=2)\n",
    "#         kd = F.kl_div(log_p_s, p_t, reduction=\"batchmean\") * (T*T)\n",
    "#     else:\n",
    "#         kd = torch.tensor(0., device=ce.device)\n",
    "\n",
    "#     return alpha * ce + beta * fm + gamma * kd, ce, fm, kd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce55b096",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a740a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_distill(train_loader, val_loader, \n",
    "                       teacher, student, \n",
    "                       criterion, metric, \n",
    "                       optimizer, cfg, \n",
    "                       scheduler = None, \n",
    "                       start_epoch=1, \n",
    "                       best_val_miou=0.0,\n",
    "                       device=\"cuda\"):\n",
    "    \n",
    "    print(f\"Starting training from epoch {start_epoch} until {cfg.train.num_epochs}\")\n",
    "    print(f\"Using batch size {cfg.train.batch_size} with {cfg.train.accum_steps} accumulation steps...\")\n",
    "\n",
    "    T = cfg.distill.temperature\n",
    "    alpha = cfg.distill.alpha\n",
    "\n",
    "    for epoch in range(start_epoch, cfg.train.num_epochs + 1):\n",
    "        ####### TRAINING #######\n",
    "        student.train()\n",
    "        metric.reset()\n",
    "        optimizer.zero_grad()\n",
    "        running_train_loss = 0.0\n",
    "        running_ce_loss = 0.0\n",
    "        running_kd_loss = 0.0\n",
    "\n",
    "        # Iterate over the training dataset\n",
    "        train_bar = tqdm(train_loader, desc=f\"[Epoch {epoch}] Train\")\n",
    "        for batch_idx, (imgs, masks) in enumerate(train_bar):\n",
    "            imgs = imgs.permute(0, 3, 1, 2)  # [B, H, W, C] -> [B, C, H, W]\n",
    "            input = student.process(imgs).to(device)\n",
    "\n",
    "            # 1) teacher forward (no grad)\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(input)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            student_logits = student(input)\n",
    "\n",
    "            # CE loss on student logits\n",
    "            masks = masks.to(device)  # [B, H, W]\n",
    "            ce_loss = criterion(student_logits, masks.long())\n",
    "            running_ce_loss += ce_loss.item()\n",
    "\n",
    "            # KL loss on softened logits\n",
    "            #    reshape to [B, H*W, C] so softmax is over classes\n",
    "            B, C, H, W = student_logits.shape\n",
    "            s = student_logits.view(B, C, -1) / T\n",
    "            t = teacher_logits.view(B, C, -1) / T\n",
    "            log_p_s = F.log_softmax(s, dim=1)\n",
    "            p_t     = F.softmax(t, dim=1)\n",
    "            kd_loss = F.kl_div(log_p_s, p_t, reduction='batchmean') * (T*T)\n",
    "            running_kd_loss += kd_loss.item()\n",
    "\n",
    "            # Combine losses\n",
    "            loss = alpha * ce_loss + (1 - alpha) * kd_loss\n",
    "\n",
    "            # scale the loss down so that gradients accumulate correctly\n",
    "            (loss / cfg.train.accum_steps).backward()\n",
    "\n",
    "            # Step and zero grad every accum_steps or at the end of epoch\n",
    "            is_accum_step = (batch_idx + 1) % cfg.train.accum_steps == 0\n",
    "            is_last_batch = batch_idx == len(train_loader) - 1\n",
    "            if is_accum_step or is_last_batch:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # accumulate losses\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            # compute IoU\n",
    "            preds = torch.argmax(student_logits, dim=1)  # [B, H, W]\n",
    "            metric.update(preds, masks)\n",
    "\n",
    "            train_bar.set_postfix(\n",
    "                loss=running_train_loss / (batch_idx + 1),\n",
    "                ce_loss=running_ce_loss / (batch_idx + 1),\n",
    "                kd_loss=running_kd_loss / (batch_idx + 1),\n",
    "            )\n",
    "\n",
    "        avg_ce_loss = running_ce_loss / len(train_loader)\n",
    "        avg_kd_loss = running_kd_loss / len(train_loader)\n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        avg_train_miou = metric.compute().item()\n",
    "\n",
    "        ####### VALIDATION #######\n",
    "        student.eval()\n",
    "        running_val_loss = 0.0\n",
    "        metric.reset()\n",
    "\n",
    "        # Prepare lists for storing predictions and targets for confusion matrix\n",
    "        if cfg.wandb.enabled:\n",
    "            all_preds = []\n",
    "            all_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(val_loader, desc=f\"[Epoch {epoch}/{cfg.train.num_epochs}]  Val\")\n",
    "            for batch_idx, (imgs, masks) in enumerate(val_bar):\n",
    "                imgs = imgs.permute(0, 3, 1, 2)  # [B, H, W, C] -> [B, C, H, W]\n",
    "                input = student.process(imgs).to(device)\n",
    "\n",
    "                # forward + loss\n",
    "                logits = student(input)\n",
    "                cls_map = None\n",
    "\n",
    "                # Loss\n",
    "                masks = masks.to(device)  # [B, H, W]\n",
    "                loss = criterion(logits, masks.long())\n",
    "\n",
    "                # accumulate losses\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "                # compute IoU on this batch\n",
    "                preds = torch.argmax(logits, dim=1)  # [B, H, W]\n",
    "                metric.update(preds, masks)\n",
    "\n",
    "                # Store predictions and targets for confusion matrix\n",
    "                if cfg.wandb.enabled:\n",
    "                    preds_np   = preds.view(-1).cpu().numpy()\n",
    "                    targets_np = masks.view(-1).cpu().numpy()\n",
    "                    valid_idx  = targets_np != 255       # drop ignore_index\n",
    "                    all_preds.extend(preds_np[valid_idx].tolist())\n",
    "                    all_targets.extend(targets_np[valid_idx].tolist())\n",
    "\n",
    "                # Log plots for the first batch\n",
    "                if batch_idx == 0 and cfg.wandb.enabled:\n",
    "                    plot_image_and_masks(\n",
    "                        imgs[0].permute(1, 2, 0).cpu().numpy(),  # Original image\n",
    "                        masks[0].cpu().numpy(),                  # Ground truth\n",
    "                        preds[0].cpu().numpy(),                  # Predicted segmentation\n",
    "                        cls_map,                                 # Attention map\n",
    "                        epoch, cfg.dataset.num_classes\n",
    "                    )\n",
    "                    if cfg.visualization.attention:\n",
    "                        del attentions\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                val_bar.set_postfix(val_loss=running_val_loss / (batch_idx + 1))\n",
    "\n",
    "        avg_val_loss = running_val_loss / len(val_loader)\n",
    "        avg_val_miou = metric.compute().item()\n",
    "\n",
    "        # Update learning rate based on validation loss\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        ####### LOG TO W&B #######\n",
    "        if cfg.wandb.enabled:\n",
    "            # Log confusion matrix\n",
    "            class_names = [trainId2label[i].name for i in range(NUM_CLASSES)]\n",
    "            confmat = wandb.plot.confusion_matrix(\n",
    "                probs=None,\n",
    "                y_true=all_targets,\n",
    "                preds=all_preds,\n",
    "                class_names=class_names\n",
    "            )\n",
    "\n",
    "            # Log metrics\n",
    "            wandb.log({\n",
    "                \"Train Loss\": avg_train_loss,\n",
    "                \"Train CE Loss\": avg_ce_loss,\n",
    "                \"Train KD Loss\": avg_kd_loss,\n",
    "                \"Train mIoU\": avg_train_miou,\n",
    "                \"Epoch\": epoch,\n",
    "                \"Validation Loss\": avg_val_loss,\n",
    "                \"Validation mIoU\": avg_val_miou,\n",
    "                \"Learning Rate\": optimizer.param_groups[0]['lr'],\n",
    "                \"Validation Confusion Matrix\": confmat\n",
    "            })\n",
    "\n",
    "        ####### PRINT & CHECKPOINT #######\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | Learning Rate: {optimizer.param_groups[0]['lr']:.6f} | \"\n",
    "            f\"\\n  Train CE Loss: {avg_ce_loss:.4f} | Train KD Loss: {avg_kd_loss:.4f} \"\n",
    "            f\"\\n  Train Loss: {avg_train_loss:.4f} | mIoU: {avg_train_miou:.4f} \"\n",
    "            f\"\\n  Val   Loss: {avg_val_loss:.4f} | mIoU: {avg_val_miou:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_miou > best_val_miou:\n",
    "            best_val_miou = avg_val_miou\n",
    "            save_checkpoint(student, optimizer, epoch, best_val_miou, cfg.checkpoint, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5c5f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "miou_metric = miou_metric.to(device)\n",
    "teacher = teacher.to(device)\n",
    "teacher.eval()\n",
    "for param in teacher.parameters():\n",
    "    param.requires_grad = False\n",
    "student = student.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a582047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from epoch 1 until 20\n",
      "Using batch size 8 with 4 accumulation steps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train:   0%|          | 0/881 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 142.94 MiB is free. Including non-PyTorch memory, this process has 3.48 GiB memory in use. Of the allocated memory 3.35 GiB is allocated by PyTorch, and 63.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_and_distill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mteacher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstudent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmiou_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                  \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 39\u001b[0m, in \u001b[0;36mtrain_and_distill\u001b[0;34m(train_loader, val_loader, teacher, student, criterion, metric, optimizer, cfg, scheduler, start_epoch, best_val_miou, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m student_logits \u001b[38;5;241m=\u001b[39m \u001b[43mstudent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# CE loss on student logits\u001b[39;00m\n\u001b[1;32m     42\u001b[0m masks \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# [B, H, W]\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/dev/hf_seg/models/student.py:135\u001b[0m, in \u001b[0;36mStudent.forward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m    132\u001b[0m B, _, H_orig, W_orig \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# backbone → [B, 1+N, C] and attention maps\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m hstates \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mhidden_states \u001b[38;5;66;03m# [13] [B, 1+N, D]\u001b[39;00m\n\u001b[1;32m    137\u001b[0m taps \u001b[38;5;241m=\u001b[39m [hstates[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idxs]\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/transformers/models/dinov2/modeling_dinov2.py:680\u001b[0m, in \u001b[0;36mDinov2Model.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    676\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    678\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos)\n\u001b[0;32m--> 680\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    688\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(sequence_output)\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/transformers/models/dinov2/modeling_dinov2.py:492\u001b[0m, in \u001b[0;36mDinov2Encoder.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    485\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    486\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    487\u001b[0m         hidden_states,\n\u001b[1;32m    488\u001b[0m         layer_head_mask,\n\u001b[1;32m    489\u001b[0m         output_attentions,\n\u001b[1;32m    490\u001b[0m     )\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 492\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/transformers/models/dinov2/modeling_dinov2.py:448\u001b[0m, in \u001b[0;36mDinov2Layer.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# in Dinov2, layernorm is also applied after self-attention\u001b[39;00m\n\u001b[1;32m    447\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(hidden_states)\n\u001b[0;32m--> 448\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_scale2(layer_output)\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# second residual connection\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/transformers/models/dinov2/modeling_dinov2.py:385\u001b[0m, in \u001b[0;36mDinov2MLP.forward\u001b[0;34m(self, hidden_state)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_state: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 385\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(hidden_state)\n\u001b[1;32m    387\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(hidden_state)\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 142.94 MiB is free. Including non-PyTorch memory, this process has 3.48 GiB memory in use. Of the allocated memory 3.35 GiB is allocated by PyTorch, and 63.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train_and_distill(train_loader, val_loader, \n",
    "                  teacher, student, \n",
    "                  criterion, miou_metric, \n",
    "                  optimizer, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b452b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
