{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8af6539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project directory: /home/panos/dev/hf_seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/panos/dev/hf_seg/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import onnx\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import onnxruntime as ort\n",
    "from onnxruntime.quantization import quantize_dynamic, quantize_static, CalibrationDataReader, QuantType\n",
    "from onnxruntime.quantization.shape_inference import quant_pre_process\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "from hydra import initialize, compose\n",
    "\n",
    "# Add project root to path\n",
    "cur_dir = Path.cwd()\n",
    "project_dir = cur_dir.parent\n",
    "print(f\"Project directory: {project_dir}\")\n",
    "sys.path.append(str(project_dir))\n",
    "\n",
    "from data.dataset import KittiSemSegDataset\n",
    "from models.DinoFPNbn import DinoFPN\n",
    "from src.compare import create_qat_model, load_qat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "845915bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18.0\n"
     ]
    }
   ],
   "source": [
    "print(onnx.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c833f7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CPU'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58f05a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize config\n",
    "with initialize(version_base=None, config_path=\"../configs\", job_name=\"onnx_export\"):\n",
    "    cfg = compose(config_name=\"config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e3af289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example input (batch_size=1, channels=3, height=364, width=1232)\n",
    "B = 1\n",
    "C = 3\n",
    "example_input = torch.randn(B, C, cfg.augmentation.crop_height, cfg.augmentation.crop_width)\n",
    "\n",
    "# Export paths\n",
    "onnx_dir = project_dir / \"onnx_models\"\n",
    "onnx_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f275c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_checkpoint_path = project_dir / \"checkpoints\" / \"dino-fpn-bn.pth\"\n",
    "fp32_onnx_path = onnx_dir / \"fp32.onnx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c67b95e",
   "metadata": {},
   "source": [
    "# Export Original FP32 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba1dc0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Loading and exporting original FP32 model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DinoFPN(\n",
       "  (backbone): Dinov2Model(\n",
       "    (embeddings): Dinov2Embeddings(\n",
       "      (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Dinov2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x Dinov2Layer(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attention): Dinov2Attention(\n",
       "            (attention): Dinov2SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Dinov2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (layer_scale1): Dinov2LayerScale()\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Dinov2MLP(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_scale2): Dinov2LayerScale()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (head): FPNHead(\n",
       "    (projs): ModuleList(\n",
       "      (0-3): 4 x Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Dropout2d(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fuse): Sequential(\n",
       "      (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout2d(p=0.1, inplace=False)\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout2d(p=0.1, inplace=False)\n",
       "      (4): Conv2d(256, 33, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"ðŸ”„ Loading and exporting original FP32 model...\")\n",
    "fp32_model = DinoFPN(num_labels=cfg.dataset.num_classes, model_cfg=cfg.model)\n",
    "checkpoint = torch.load(fp32_checkpoint_path)\n",
    "fp32_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "fp32_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68794850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 364, 1232])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process input through model preprocessing\n",
    "processed_input = fp32_model.process(example_input)\n",
    "processed_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba829919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnx_program = torch.onnx.export(\n",
    "#     fp32_model, \n",
    "#     processed_input,\n",
    "#     dynamo=True,\n",
    "#     export_params=True,\n",
    "#     do_constant_folding=True,\n",
    "#     input_names=['input'],\n",
    "#     output_names=['output'],\n",
    "#     dynamic_axes={\n",
    "#         'input': {0: 'batch_size'},\n",
    "#         'output': {0: 'batch_size'}\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7aa0ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize ONNX model\n",
    "# onnx_program.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acb50247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the optimized ONNX model\n",
    "# onnx_program.save(fp32_onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54b9cf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model IR version: 10\n",
      "Opset version: 18\n",
      "ONNX model is valid.\n"
     ]
    }
   ],
   "source": [
    "# Load and check the exported ONNX model\n",
    "onnx_model = onnx.load(fp32_onnx_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"Model IR version:\", onnx_model.ir_version)\n",
    "print(\"Opset version:\", onnx_model.opset_import[0].version)\n",
    "print(\"ONNX model is valid.\")\n",
    "# I can also visualize the model using Netron: https://netron.app/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19d4b8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 364, 1232])\n",
      "Input shape: torch.Size([1, 3, 364, 1232])\n"
     ]
    }
   ],
   "source": [
    "# Execute the model using ONNX Runtime\n",
    "print(processed_input.shape)\n",
    "input = processed_input  # Add batch dimension\n",
    "print(\"Input shape:\", input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c05d928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Creating ONNX Runtime session from /home/panos/dev/hf_seg/onnx_models/fp32.onnx...\n"
     ]
    }
   ],
   "source": [
    "# Create ONNX Runtime session\n",
    "print(f\"ðŸ”„ Creating ONNX Runtime session from {fp32_onnx_path}...\")\n",
    "ort_session = ort.InferenceSession(\n",
    "    path_or_bytes=fp32_onnx_path, \n",
    "    providers=[\"CPUExecutionProvider\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d3106bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inputs: 1\n",
      "First input shape: (1, 3, 364, 1232)\n"
     ]
    }
   ],
   "source": [
    "# Prepare inputs for ONNX Runtime\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "onnxruntime_input = {input_name: input.numpy()}\n",
    "\n",
    "print(f\"Number of inputs: {len(onnxruntime_input)}\")\n",
    "first_input_shape = onnxruntime_input[\"input\"].shape\n",
    "print(f\"First input shape: {first_input_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd7f1a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference with ONNX Runtime...\n",
      "ONNX Runtime inference time: 2.928489 seconds\n"
     ]
    }
   ],
   "source": [
    "# ONNXRuntime inference and time it\n",
    "print(\"Running inference with ONNX Runtime...\")\n",
    "t0 = time.perf_counter()\n",
    "onnxruntime_outputs = ort_session.run(None, onnxruntime_input)[0]\n",
    "t1 = time.perf_counter()\n",
    "print(f\"ONNX Runtime inference time: {t1 - t0:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02404f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outputs: 1\n",
      "First output shape: (33, 364, 1232)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of outputs: {len(onnxruntime_outputs)}\")\n",
    "first_output_shape = onnxruntime_outputs[0].shape\n",
    "print(f\"First output shape: {first_output_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee5de54",
   "metadata": {},
   "source": [
    "# Export QAT Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e95889b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"ðŸ”„ Loading and exporting QAT quantized model...\")\n",
    "# qat_model_path = project_dir / \"checkpoints\" / \"dino-fpn-qat-int8.pth\"\n",
    "# qat_model = load_qat_model(qat_model_path, fp32_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25836a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qat_input = processed_input.squeeze(0)  # Add batch dimension\n",
    "# qat_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63989b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qat_onnx_path = onnx_dir / \"dino_fpn_qat_int8.onnx\"\n",
    "# onnx_qat_program = torch.onnx.export(\n",
    "#     qat_model, \n",
    "#     qat_input,\n",
    "#     dynamo=True,\n",
    "#     export_params=True,\n",
    "#     opset_version=17,\n",
    "#     do_constant_folding=False,\n",
    "#     input_names=['input'],\n",
    "#     output_names=['logits']\n",
    "# )\n",
    "# print(f\"\\nQAT model exported to {onnx_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f958fea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optimize ONNX model\n",
    "# onnx_qat_program.optimize()\n",
    "\n",
    "# # save the optimized ONNX model\n",
    "# onnx_qat_program.save(qat_onnx_path)\n",
    "\n",
    "# # Load and check the exported ONNX model\n",
    "# onnx_qat_model = onnx.load(qat_onnx_path)\n",
    "# onnx.checker.check_model(onnx_qat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acfaa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create ONNX Runtime session\n",
    "# ort_qat_session = ort.InferenceSession(\n",
    "#     path_or_bytes=qat_onnx_path, \n",
    "#     providers=[\"CPUExecutionProvider\"]\n",
    "# )\n",
    "\n",
    "# # ONNXRuntime inference and time it\n",
    "# print(\"Running inference with ONNX Runtime...\")\n",
    "# t0 = time.perf_counter()\n",
    "# onnxruntime_outputs = ort_qat_session.run(None, onnxruntime_input)[0]\n",
    "# t1 = time.perf_counter()\n",
    "# print(f\"ONNX Runtime inference time: {t1 - t0:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d265b309",
   "metadata": {},
   "source": [
    "# Quantize with ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare the model for quantization\n",
    "# model_prep_path = onnx_dir / \"fp32_prep.onnx\"\n",
    "# quant_pre_process(\n",
    "#     fp32_onnx_path, \n",
    "#     model_prep_path,\n",
    "#     skip_symbolic_shape=True,\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0aa644f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QuantPTQ] Calibration dataset size: 704\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "\n",
    "crop_h, crop_w = (cfg.augmentation.crop_height, cfg.augmentation.crop_width)\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.CenterCrop(crop_h, crop_w)\n",
    "])\n",
    "\n",
    "cal_dataset = KittiSemSegDataset(\n",
    "    root_dir='/home/panos/Documents/data/kitti-360',\n",
    "    train=True,\n",
    "    calibration=True,\n",
    "    transform=val_transform\n",
    ")\n",
    "cal_loader = DataLoader(\n",
    "    cal_dataset,\n",
    "    batch_size=cfg.train.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=cfg.dataset.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "print(f\"[QuantPTQ] Calibration dataset size: {len(cal_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94156de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizationDataReader(CalibrationDataReader):\n",
    "    def __init__(self, torch_dl, input_name):\n",
    "\n",
    "        self.torch_dl = torch_dl\n",
    "        self.input_name = input_name\n",
    "\n",
    "        self.datasize = len(self.torch_dl)\n",
    "        self.dl_iter = iter(self.torch_dl)\n",
    "\n",
    "    def to_numpy(self, pt_tensor):\n",
    "        return pt_tensor.detach().cpu().numpy() if pt_tensor.requires_grad else pt_tensor.cpu().numpy()\n",
    "\n",
    "    def get_next(self):\n",
    "        batch = next(self.dl_iter) # [2] [B, C, H, W] [B, H, W]\n",
    "        imgs = batch[0]  # [B, C, H, W]\n",
    "        masks = batch[1] # [B, H, W]\n",
    "        processed_imgs = fp32_model.process(imgs) if batch is not None else None\n",
    "        print(\"Batch shape:\", processed_imgs.shape)\n",
    "        if batch is not None:\n",
    "          print(f\"Type of processed_imgs: {type(processed_imgs)}\")\n",
    "          input = self.to_numpy(processed_imgs)\n",
    "          print(f\"[QuantPTQ] Input type: {type(input)}\")\n",
    "          print(f\"[QuantPTQ] Input shape: {input.shape}\")\n",
    "          return {self.input_name: input}\n",
    "        else:\n",
    "          return None\n",
    "\n",
    "    def rewind(self):\n",
    "        self.dl_iter = iter(self.torch_dl)\n",
    "\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "qdr = QuantizationDataReader(cal_loader, input_name=input_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d74910b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QuantPTQ] Batch size: 2\n",
      "torch.Size([12, 364, 1232, 3]) torch.Size([12, 364, 1232])\n",
      "[QuantPTQ] Batch type: <class 'list'>\n",
      "Batch shjape: torch.Size([12, 364, 1232, 3])\n",
      "Batch shape: torch.Size([12, 3, 364, 1232])\n",
      "Type of processed_batch: <class 'torch.Tensor'>\n",
      "[QuantPTQ] Input type: <class 'numpy.ndarray'>\n",
      "[QuantPTQ] Input shape: (12, 3, 364, 1232)\n",
      "Calibration data type: <class 'numpy.ndarray'>, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "inputs = qdr.get_next()\n",
    "print(f\"Calibration data type: {type(inputs[qdr.input_name])}, dtype: {inputs[qdr.input_name].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1554a9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ActivationSymmetric': True, 'WeightSymmetric': True}\n"
     ]
    }
   ],
   "source": [
    "q_static_opts = {\"ActivationSymmetric\":False,\n",
    "                 \"WeightSymmetric\":True}\n",
    "if torch.cuda.is_available():\n",
    "    q_static_opts = {\"ActivationSymmetric\":True,\n",
    "                    \"WeightSymmetric\":True}\n",
    "print(q_static_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b46a0b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input name: input, type: tensor(float)\n"
     ]
    }
   ],
   "source": [
    "for input_meta in ort_session.get_inputs():\n",
    "    print(f\"Input name: {input_meta.name}, type: {input_meta.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59829bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([1, 3, 364, 1232])\n",
      "[QuantPTQ] Input shape: (1, 3, 364, 1232)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_prep_path = onnx_dir / \"fp32_prep.onnx\"\n",
    "model_int8_path = onnx_dir / \"int8.onnx\"\n",
    "quantized_model = quantize_static(model_input=model_prep_path,\n",
    "                                  model_output=model_int8_path,\n",
    "                                  calibration_data_reader=qdr,\n",
    "                                  extra_options=q_static_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce0d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4ea614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b014706e",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dd8979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image 1/783\n",
      "Input shape: torch.Size([1, 3, 364, 1232])\n",
      "Original Inference time: 2.3400 sec\n",
      "Original mIoU = 0.9658\n",
      "ONNX Runtime inference time: 3.1420 sec\n",
      "ONNX Logits shape: (1, 33, 364, 1232)\n",
      "ONNX Predictions shape: (1, 364, 1232)\n",
      "ONNX mIoU = 0.9658\n",
      "Image shape: (364, 1232, 3)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m mask_np \u001b[38;5;241m=\u001b[39m masks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     75\u001b[0m pred_np \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 76\u001b[0m onnx_pred_np \u001b[38;5;241m=\u001b[39m \u001b[43monnx_preds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     78\u001b[0m fig, (ax1, ax2, ax3, ax4) \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     79\u001b[0m ax1\u001b[38;5;241m.\u001b[39mimshow(img_np)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "import numpy as np\n",
    "from torchmetrics import JaccardIndex\n",
    "import albumentations as A\n",
    "\n",
    "NUM_CLASSES = cfg.dataset.num_classes\n",
    "\n",
    "dataset_root = '/home/panos/Documents/data/kitti-360'\n",
    "val_transform = A.Compose([A.CenterCrop(cfg.augmentation.crop_height, cfg.augmentation.crop_width)])\n",
    "val_dataset = KittiSemSegDataset(dataset_root, train=False, transform=val_transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "cmap = plt.get_cmap(\"viridis\", NUM_CLASSES)\n",
    "norm = BoundaryNorm(boundaries=np.arange(NUM_CLASSES + 1) - 0.5, ncolors=NUM_CLASSES)\n",
    "\n",
    "# Metric: mean IoU\n",
    "miou_metric = JaccardIndex(\n",
    "    task='multiclass',\n",
    "    num_classes=NUM_CLASSES,\n",
    "    average='micro',\n",
    "    ignore_index=None\n",
    ")\n",
    "\n",
    "fp32_model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, (imgs, masks) in enumerate(val_loader):\n",
    "        masks = masks  # [B, H, W]\n",
    "        imgs = imgs.permute(0, 3, 1, 2)  # [B, H, W, C] -> [B, C, H, W]\n",
    "        input = fp32_model.process(imgs)  # Preprocess input for the model\n",
    "        print(f\"\\nProcessing image {idx + 1}/{len(val_loader)}\")\n",
    "\n",
    "        # Forward pass with inference time\n",
    "        t0 = time.perf_counter()\n",
    "        print(f\"Input shape: {input.shape}\")\n",
    "        logits = fp32_model(input)\n",
    "        t1 = time.perf_counter()\n",
    "        inference_time = t1 - t0\n",
    "        print(f\"Original Inference time: {inference_time:.4f} sec\") # ~0.45 seconds\n",
    "\n",
    "        # Compute mIoU for the current image\n",
    "        preds = torch.argmax(logits, dim=1)  # [B, H, W]\n",
    "        miou_metric.reset()\n",
    "        miou_metric.update(preds, masks)\n",
    "        miou = miou_metric.compute().item()\n",
    "        print(f\"Original mIoU = {miou:.4f}\")\n",
    "\n",
    "        # Prepare the input for ONNX Runtime\n",
    "        processed_input = input.unsqueeze(0)  # Add batch dimension\n",
    "        onnx_inputs = [tensor.numpy(force=True) for tensor in processed_input]\n",
    "        onnxruntime_input = {input_arg.name: input_value for input_arg, input_value in zip(ort_session.get_inputs(), onnx_inputs)}\n",
    "\n",
    "        # Forward pass with ONNX Runtime\n",
    "        t2 = time.perf_counter()\n",
    "        onnx_logits = ort_session.run(None, onnxruntime_input)[0]\n",
    "        t3 = time.perf_counter()\n",
    "        ort_inference_time = t3 - t2\n",
    "        print(f\"ONNX Runtime inference time: {ort_inference_time:.4f} sec\")  # ~0.35 seconds\n",
    "        print(f\"ONNX Logits shape: {onnx_logits.shape}\")\n",
    "\n",
    "        # Extract predictions from ONNX logits\n",
    "        onnx_preds = np.argmax(onnx_logits, axis=1)  # [B, H, W]\n",
    "        print(f\"ONNX Predictions shape: {onnx_preds.shape}\")\n",
    "\n",
    "        # Compute mIoU for ONNX predictions\n",
    "        miou_metric.reset()\n",
    "        miou_metric.update(torch.tensor(onnx_preds), masks)\n",
    "        onnx_miou = miou_metric.compute().item()\n",
    "        print(f\"ONNX mIoU = {onnx_miou:.4f}\")\n",
    "\n",
    "        # Plot the image, ground truth, and prediction\n",
    "        img_np = imgs[0].permute(1, 2, 0)  # Convert to HWC format\n",
    "        print(f\"Image shape: {img_np.shape}\")\n",
    "        mask_np = masks[0]\n",
    "        pred_np = preds[0]\n",
    "        onnx_pred_np = onnx_preds[0]\n",
    "\n",
    "        fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20, 5))\n",
    "        ax1.imshow(img_np)\n",
    "        ax1.set_title(\"Input Image\")\n",
    "        ax1.axis(\"off\")\n",
    "\n",
    "        ax2.imshow(mask_np, cmap=cmap, norm=norm)\n",
    "        ax2.set_title(\"Ground Truth\")\n",
    "        ax2.axis(\"off\")\n",
    "\n",
    "        ax3.imshow(pred_np, cmap=cmap, norm=norm)\n",
    "        ax3.set_title(f\"Prediction (mIoU={miou:.4f})\")\n",
    "        ax3.axis(\"off\")\n",
    "\n",
    "        ax4.imshow(onnx_pred_np, cmap=cmap, norm=norm)\n",
    "        ax4.set_title(f\"ONNX Prediction (mIoU={onnx_miou:.4f})\")\n",
    "        ax4.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()  # Wait for the user to close the plot before continuing\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f2ddb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
