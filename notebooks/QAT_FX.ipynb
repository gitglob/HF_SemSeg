{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c909fffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/panos/dev/hf_seg/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import JaccardIndex\n",
    "import albumentations as A\n",
    "from hydra import initialize, compose\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a177f067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Make sure your project root is in PYTHONPATH so we can import models & datasets\n",
    "cur_dir     = Path.cwd()\n",
    "project_dir = cur_dir.parent\n",
    "sys.path.append(str(project_dir))\n",
    "\n",
    "from models.DinoFPNbn import DinoFPN\n",
    "from data.dataset import KittiSemSegDataset\n",
    "from data.labels_kitti360 import NUM_CLASSES\n",
    "from utils.others import get_memory_footprint, get_quant_memory_footprint\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"CUDA is available.\")\n",
    "else:\n",
    "    print(\"CUDA is not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a6e5ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Achieves 83.94% mIoU on KITTI-360 validation set with QAT'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Achieves 83.94% mIoU on KITTI-360 validation set with QAT\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ff75e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 5)\n",
      "['qnnpack', 'none', 'onednn', 'x86', 'fbgemm']\n"
     ]
    }
   ],
   "source": [
    "# Check if hardward supports activation quantization\n",
    "print((torch.cuda.get_device_properties(device).major, torch.cuda.get_device_properties(device).minor))\n",
    "print(torch.backends.quantized.supported_engines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64144fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hydra config\n",
    "with initialize(version_base=None, config_path=f\"../configs\", job_name=\"quant_static_ptq\"):\n",
    "    cfg = compose(config_name=\"qat_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2c476a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_model = DinoFPN(num_labels=cfg.dataset.num_classes, model_cfg=cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da23dfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Memory Footprint ===\n",
      "Backbone: 86,580,480 params, 330.28 MB\n",
      "Head:     3,747,617 params, 14.30 MB\n",
      "Total:    90,328,097 params, 344.57 MB\n"
     ]
    }
   ],
   "source": [
    "fp32_mem_footprint = get_memory_footprint(fp32_model, detailed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7c0aa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.ao.quantization import fuse_modules\n",
    "# 2. (Optional) Fuse modules manually if you want more control\n",
    "# fuse_modules(model, [['features.0', 'features.1', 'features.2']], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0208bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization import get_default_qconfig, QConfig\n",
    "from torch.ao.quantization import QConfigMapping\n",
    "from torch.ao.quantization.observer import MovingAverageMinMaxObserver\n",
    "from torch.ao.quantization.observer import PerChannelMinMaxObserver, HistogramObserver\n",
    "from torch.ao.quantization.fake_quantize import FakeQuantize\n",
    "from torch.ao.quantization._learnable_fake_quantize import _LearnableFakeQuantize as LearnableFakeQuantize\n",
    "import torch.quantization as tq\n",
    "\n",
    "# Build a MinMax observer for weights\n",
    "weight_obs = PerChannelMinMaxObserver.with_args(\n",
    "    dtype=torch.qint8,\n",
    "    qscheme=torch.per_channel_symmetric,\n",
    "    ch_axis=0,\n",
    "    reduce_range=False,\n",
    ")\n",
    "learnable_qat_weight = LearnableFakeQuantize.with_args(\n",
    "    observer=weight_obs,\n",
    "    quant_min=-128,\n",
    "    quant_max=127,\n",
    "    dtype=torch.qint8,\n",
    "    qscheme=torch.per_channel_symmetric,\n",
    "    ch_axis=0,\n",
    "    scale=1.0,        # Initial scale (will be learned)\n",
    "    zero_point=0.0    # Initial zero point (will be learned)\n",
    ")\n",
    "qat_weight = FakeQuantize.with_args(\n",
    "    observer=weight_obs\n",
    ")\n",
    "\n",
    "# - Build a histogram (KL) observer for activations\n",
    "activation_obs = HistogramObserver.with_args(\n",
    "    dtype=torch.quint8,\n",
    "    qscheme=torch.per_tensor_affine,\n",
    "    reduce_range=False\n",
    ")\n",
    "\n",
    "learnable_qat_activation = LearnableFakeQuantize.with_args(\n",
    "    observer=activation_obs,\n",
    "    quant_min=0,\n",
    "    quant_max=255,\n",
    "    dtype=torch.quint8,\n",
    "    qscheme=torch.per_tensor_affine,\n",
    "    reduce_range=False,\n",
    "    scale=1.0,             # Initial scale (will be learned)\n",
    "    zero_point=128.0       # Initial zero point (will be learned)\n",
    ")\n",
    "qat_activation = FakeQuantize.with_args(\n",
    "    observer=activation_obs\n",
    ")\n",
    "\n",
    "custom_qconfig = QConfig(\n",
    "    activation=qat_activation, \n",
    "    weight=tq.default_fused_per_channel_wt_fake_quant\n",
    ")\n",
    "# custom_qconfig = get_default_qconfig(\"x86\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75821be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "qconfig_map = (\n",
    "    QConfigMapping()\n",
    "    .set_global(custom_qconfig)                   # applies to all modules by default\n",
    "    # .set_module_name(\"backbone.embeddings\", None)  # disable quant for embeddings\n",
    "    .set_module_name(\"backbone.embeddings.patch_embeddings\", None)\n",
    "    .set_module_name(\"backbone.embeddings.dropout\", None)\n",
    "    # Exclude normalization layers\n",
    "    .set_object_type(torch.nn.LayerNorm, None)\n",
    "    .set_object_type(torch.nn.BatchNorm2d, None)\n",
    "    .set_object_type(torch.nn.GroupNorm, None)\n",
    "    # Exclude operations that return non-tensor objects\n",
    "    .set_object_type(\"size\", None)\n",
    "    .set_object_type(\"view\", None)\n",
    "    .set_object_type(\"reshape\", None)\n",
    "    .set_object_type(\"permute\", None)\n",
    "    .set_object_type(torch.Tensor.size, None)\n",
    "    .set_object_type(torch.Tensor.view, None)\n",
    "    .set_object_type(torch.Tensor.reshape, None)\n",
    ")\n",
    "\n",
    "from torch.ao.quantization.fx.custom_config import PrepareCustomConfig\n",
    "from transformers.models.dinov2.modeling_dinov2 import Dinov2PatchEmbeddings, Dinov2Embeddings\n",
    "\n",
    "# Tell FX to treat Dinov2PatchEmbeddings as non-traceable\n",
    "## ‚ÄúWhenever you hit SomeModule in the model, don‚Äôt open it up and record its internal steps. \n",
    "## Instead just treat the whole call as a single step in the recipe\n",
    "prepare_custom_config = (\n",
    "    PrepareCustomConfig()\n",
    "    .set_non_traceable_module_classes([\n",
    "        Dinov2PatchEmbeddings, \n",
    "        Dinov2Embeddings\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c08919aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FP32 weights from /home/panos/dev/hf_seg/checkpoints/dino-fpn-bn.pth into fp32_model.\n"
     ]
    }
   ],
   "source": [
    "# Load your best FP32 checkpoint into `quant_model.fp32_model`\n",
    "ckpt_path = os.path.join(project_dir, \"checkpoints\", f\"{cfg.checkpoint.model_name}.pth\")\n",
    "if not os.path.exists(ckpt_path):\n",
    "    print(f\"[Error] Checkpoint not found: {ckpt_path}\")\n",
    "checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "fp32_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "print(f\"Loaded FP32 weights from {ckpt_path} into fp32_model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90f122cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization.quantize_fx import prepare_qat_fx\n",
    "\n",
    "# 4. Prepare the model for static quantization\n",
    "example_inputs = (torch.randn(1, 3, 364, 1232),)\n",
    "prep_model = prepare_qat_fx(\n",
    "    fp32_model, \n",
    "    qconfig_map, \n",
    "    example_inputs,\n",
    "    prepare_custom_config=prepare_custom_config\n",
    ")\n",
    "# prep_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bf0b800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_qconfig_status(model):\n",
    "    \"\"\"Print qconfig status for all modules\"\"\"\n",
    "    print(\"\\n=== QConfig Status for All Modules ===\")\n",
    "    \n",
    "    quantized_modules = []\n",
    "    excluded_modules = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        qconfig = getattr(module, 'qconfig', 'not_set')\n",
    "        \n",
    "        if qconfig is None:\n",
    "            excluded_modules.append(name)\n",
    "            print(f\"{name:60s} ‚Üí EXCLUDED (qconfig=None)\")\n",
    "        elif qconfig == 'not_set':\n",
    "            print(f\"{name:60s} ‚Üí NO QCONFIG\")\n",
    "        else:\n",
    "            quantized_modules.append(name)\n",
    "            print(f\"{name:60s} ‚Üí QUANTIZED ({type(qconfig).__name__})\")\n",
    "    \n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"   Quantized modules: {len(quantized_modules)}\")\n",
    "    print(f\"   Excluded modules:  {len(excluded_modules)}\")\n",
    "    \n",
    "    return quantized_modules, excluded_modules\n",
    "\n",
    "# Use it after prepare_fx\n",
    "# quantized_modules, excluded_modules = print_qconfig_status(prep_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "388b16de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prep_model.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3678734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prep_model.graph.print_tabular())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41ca3fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.fx.passes.graph_drawer import FxGraphDrawer\n",
    "\n",
    "# # 4. Render to PNG\n",
    "# drawer = FxGraphDrawer(prep_model, \"prep_model\")\n",
    "# dot = drawer.get_dot_graph()\n",
    "# with open(project_dir / \"assets\" / \"prep_model.png\", \"wb\") as f:\n",
    "#     f.write(dot.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e8e917f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QuantQAT] Validation dataset size: 783\n",
      "[QuantQAT] Train dataset size: 7042\n"
     ]
    }
   ],
   "source": [
    "# Build a validation/calibration loader (no augmentations, just center-crop)\n",
    "crop_h, crop_w = (cfg.augmentation.crop_height, cfg.augmentation.crop_width)\n",
    "\n",
    "qat_transform = A.Compose([\n",
    "    A.CenterCrop(crop_h, crop_w)\n",
    "])\n",
    "\n",
    "val_dataset = KittiSemSegDataset(\n",
    "    root_dir='/home/panos/Documents/data/kitti-360',\n",
    "    train=False,\n",
    "    transform=qat_transform\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.dataset.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "print(f\"[QuantQAT] Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "train_dataset = KittiSemSegDataset(\n",
    "    root_dir='/home/panos/Documents/data/kitti-360',\n",
    "    train=True,\n",
    "    transform=qat_transform\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=cfg.dataset.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "print(f\"[QuantQAT] Train dataset size: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92b6d8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAT model moved to cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from models.tools import CombinedLoss\n",
    "\n",
    "# CRITICAL: Move to GPU BEFORE training\n",
    "prep_model = prep_model.to(device)\n",
    "print(f\"QAT model moved to {device}\")\n",
    "\n",
    "# Setup training components\n",
    "# criterion = CombinedLoss(alpha=0.8, ignore_index=255)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "optimizer = optim.Adam(prep_model.parameters(), lr=1e-5)  # Lower learning rate for QAT\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader))\n",
    "miou_metric = JaccardIndex(\n",
    "    task='multiclass',\n",
    "    num_classes=cfg.dataset.num_classes,\n",
    "    average='micro',\n",
    "    ignore_index=255\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c9d5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting QAT training for 5 epochs...\n",
      "QAT model moved to cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[QAT Epoch 1/5] Train:  11%|‚ñà         | 99/881 [16:58<2:14:07, 10.29s/it, loss=0.19, lr=9.69e-6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Average Train loss: 0.1900\n",
      "\t Train mIoU: 0.8773\n",
      "QAT Model moved to cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Eval]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 98/98 [22:01<00:00, 13.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QAT] INT8‚Äêquantized model mIoU = 0.8170\n",
      "Epoch 1 completed\n",
      "QAT model moved to cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[QAT Epoch 2/5] Train:   0%|          | 0/881 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 32.00 MiB is free. Including non-PyTorch memory, this process has 3.59 GiB memory in use. Of the allocated memory 3.41 GiB is allocated by PyTorch, and 107.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m fp32_model\u001b[38;5;241m.\u001b[39mprocess(imgs)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# [B, 3, H, W]\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Forward pass through QAT model\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mprep_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     27\u001b[0m masks \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/fx/graph_module.py:830\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/fx/graph_module.py:406\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# noqa: B904\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/fx/graph_module.py:393\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_call(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m<eval_with_key>.2:211\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m    209\u001b[0m backbone_encoder_layer_2_norm2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mlayer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mnorm2(activation_post_process_76)\n\u001b[1;32m    210\u001b[0m activation_post_process_77 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_post_process_77(backbone_encoder_layer_2_norm2);  backbone_encoder_layer_2_norm2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m backbone_encoder_layer_2_mlp_fc1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation_post_process_77\u001b[49m\u001b[43m)\u001b[49m;  activation_post_process_77 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    212\u001b[0m activation_post_process_78 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_post_process_78(backbone_encoder_layer_2_mlp_fc1);  backbone_encoder_layer_2_mlp_fc1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    213\u001b[0m gelu_2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mgelu(activation_post_process_78);  activation_post_process_78 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/ao/nn/quantized/reference/modules/linear.py:51\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03mwe have:\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03mw(float) -- quant - dequant \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03mand the backend should be able to fuse the ops with `*` into a quantized linear\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m weight_quant_dequant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_weight()\n\u001b[0;32m---> 51\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_quant_dequant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 32.00 MiB is free. Including non-PyTorch memory, this process has 3.59 GiB memory in use. Of the allocated memory 3.41 GiB is allocated by PyTorch, and 107.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# QAT Training Loop\n",
    "num_epochs = 5  # Usually 1-5 epochs for QAT fine-tuning\n",
    "print(f\"Starting QAT training for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    prep_model = prep_model.to(device)\n",
    "    print(f\"QAT model moved to {device}\")\n",
    "\n",
    "    ####### TRAIN #######\n",
    "    prep_model.train()\n",
    "    miou_metric.reset()\n",
    "    running_train_loss = 0.0\n",
    "\n",
    "    train_bar = tqdm(train_loader, desc=f\"[QAT Epoch {epoch+1}/{num_epochs}] Train\")\n",
    "    for batch_idx, (imgs, masks) in enumerate(train_bar, start=1):\n",
    "        if batch_idx >= 100:\n",
    "            break\n",
    "        imgs = imgs.permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Forward pass through preprocessing\n",
    "        input = fp32_model.process(imgs).to(device)  # [B, 3, H, W]\n",
    "        \n",
    "        # Forward pass through QAT model\n",
    "        logits = prep_model(input)\n",
    "        \n",
    "        # Compute loss\n",
    "        masks = masks.to(device)\n",
    "        loss = criterion(logits, masks.long())\n",
    "        running_train_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # compute IoU\n",
    "        preds = torch.argmax(logits, dim=1)  # [B, H, W]\n",
    "        miou_metric.update(preds, masks)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        train_bar.set_postfix(loss=running_train_loss / (batch_idx + 1), lr=scheduler.get_last_lr()[0])\n",
    "\n",
    "    print(f\"\\t Average Train loss: {running_train_loss/batch_idx:.4f}\")\n",
    "    print(f\"\\t Train mIoU: {miou_metric.compute().item():.4f}\")\n",
    "\n",
    "    # ####### VALIDATION #######\n",
    "    prep_model.eval()\n",
    "    miou_metric.reset()\n",
    "    running_val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(val_loader, desc=f\"[QAT Epoch {epoch + 1}/{num_epochs}]  Val\")\n",
    "        for batch_idx, (imgs, masks) in enumerate(val_bar, start=1):\n",
    "            imgs = imgs.permute(0, 3, 1, 2).to(device)  # [B, H, W, C] -> [B, C, H, W]\n",
    "            input = fp32_model.process(imgs).to(device)\n",
    "\n",
    "            # forward + loss\n",
    "            logits = prep_model(input)\n",
    "\n",
    "            # Loss\n",
    "            masks = masks.to(device)  # [B, H, W]\n",
    "            loss = criterion(logits, masks.long())\n",
    "\n",
    "            # accumulate losses\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "            # compute IoU on this batch\n",
    "            preds = torch.argmax(logits, dim=1)  # [B, H, W]\n",
    "            miou_metric.update(preds, masks)\n",
    "\n",
    "            val_bar.set_postfix(val_loss=running_val_loss / batch_idx)\n",
    "\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "    avg_val_miou = miou_metric.compute().item()\n",
    "\n",
    "    print(f\"\\t Average Val loss: {running_val_loss/len(val_loader):.4f}\")\n",
    "    print(f\"\\t Val mIoU: {miou_metric.compute().item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed\")\n",
    "\n",
    "print(\"QAT training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd574abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/panos/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/ao/quantization/observer.py:1333: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.ao.quantization.quantize_fx import convert_fx\n",
    "\n",
    "# 6. Convert: swap out float ops for quantized kernels\n",
    "prep_model = prep_model.to(\"cpu\")\n",
    "prep_model.eval()\n",
    "quant_model = convert_fx(prep_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb32885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate mIoU of the INT8 model\n",
    "with torch.no_grad():\n",
    "    miou_metric = JaccardIndex(\n",
    "        task=\"multiclass\",\n",
    "        num_classes=NUM_CLASSES,\n",
    "        average=\"micro\",\n",
    "        ignore_index=255,\n",
    "    )\n",
    "\n",
    "    for imgs, masks in tqdm(val_loader, desc=\"[Eval]\"):\n",
    "        imgs  = imgs.permute(0, 3, 1, 2).float()\n",
    "        input = fp32_model.process(imgs)\n",
    "        logits = quant_model(input)           # [B, num_classes, H, W]\n",
    "        preds  = torch.argmax(logits, dim=1)  # [B, H, W]\n",
    "        miou_metric.update(preds, masks)\n",
    "\n",
    "    int8_miou = miou_metric.compute().item()\n",
    "print(f\"[QAT] INT8‚Äêquantized model mIoU = {int8_miou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5dd283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4j) Save the INT8 state_dict\n",
    "save_path = os.path.join(project_dir, \"checkpoints\", f\"dino-fpn-qat-int8.pth\")\n",
    "torch.save(quant_model.state_dict(), save_path)\n",
    "print(f\"[QuantPTQ] Saved INT8 weights to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d77499",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cf3fb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Memory Footprint ===\n",
      "Total:    90,328,097 params, 344.57 MB\n",
      "=== Model Memory Footprint ===\n",
      "Total:     94,686,819 params, 108.13 MB\n"
     ]
    }
   ],
   "source": [
    "fp32_mem_footprint = get_memory_footprint(fp32_model)\n",
    "quant_mem_footprint = get_quant_memory_footprint(quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9d8911",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
