{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c909fffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/panos/dev/hf_seg/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/panos/dev/hf_seg/.venv/lib/python3.10/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.7'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.quantization as tq\n",
    "import torch.nn.intrinsic as nni\n",
    "from torch.quantization.observer import HistogramObserver, MinMaxObserver, PerChannelMinMaxObserver\n",
    "from torch.quantization import QConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import JaccardIndex\n",
    "import albumentations as A\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "from hydra import initialize, compose\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a177f067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n"
     ]
    }
   ],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# Make sure your project root is in PYTHONPATH so we can import models & datasets\n",
    "cur_dir     = Path.cwd()\n",
    "project_dir = cur_dir.parent\n",
    "sys.path.append(str(project_dir))\n",
    "\n",
    "from models.DinoFPNbn import DinoFPN\n",
    "from data.dataset import KittiSemSegDataset\n",
    "from data.labels_kitti360 import NUM_CLASSES\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"CUDA is available.\")\n",
    "else:\n",
    "    print(\"CUDA is not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0ff75e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 5)\n",
      "['qnnpack', 'none', 'onednn', 'x86', 'fbgemm']\n"
     ]
    }
   ],
   "source": [
    "# Check if hardward supports activation quantization\n",
    "print((torch.cuda.get_device_properties(device).major, torch.cuda.get_device_properties(device).minor))\n",
    "print(torch.backends.quantized.supported_engines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64144fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hydra config\n",
    "with initialize(version_base=None, config_path=f\"../configs\", job_name=\"quant_static_ptq\"):\n",
    "    cfg = compose(config_name=\"quant_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2c476a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DinoFPN(\n",
       "  (backbone): Dinov2Model(\n",
       "    (embeddings): Dinov2Embeddings(\n",
       "      (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Dinov2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x Dinov2Layer(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attention): Dinov2Attention(\n",
       "            (attention): Dinov2SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Dinov2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (layer_scale1): Dinov2LayerScale()\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Dinov2MLP(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_scale2): Dinov2LayerScale()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (head): FPNHead(\n",
       "    (projs): ModuleList(\n",
       "      (0-3): 4 x Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Dropout2d(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fuse): Sequential(\n",
       "      (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout2d(p=0.1, inplace=False)\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout2d(p=0.1, inplace=False)\n",
       "      (4): Conv2d(256, 33, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp32_model = DinoFPN(num_labels=cfg.dataset.num_classes, model_cfg=cfg.model)\n",
    "fp32_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da23dfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Memory Footprint ===\n",
      "Backbone: 86,580,480 params, 330.28 MB\n",
      "Head:     3,747,617 params, 14.30 MB\n",
      "Total:    90,328,097 params, 344.57 MB\n"
     ]
    }
   ],
   "source": [
    "fp32_mem_footprint = fp32_model.get_memory_footprint(detailed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01068e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Build a wrapper around the FP32 DinoFPN so we can insert QuantStubs and DeQuantStubs.\n",
    "#    Because FPNHead now uses Conv2d → BatchNorm2d → ReLU, PyTorch’s fuse_modules will handle:\n",
    "#       Conv2d + BatchNorm2d + ReLU → FusedConvBnRelu\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "class QuantDinoFPN(nn.Module):\n",
    "    def __init__(self, num_labels: int, model_cfg):\n",
    "        super().__init__()\n",
    "        # 1a) QuantStub to quantize input activations\n",
    "        self.quant = tq.QuantStub()\n",
    "\n",
    "        # 1b) The original FP32 DinoFPN (with BatchNorm)\n",
    "        self.fp32_model = DinoFPN(num_labels=num_labels, model_cfg=model_cfg)\n",
    "                \n",
    "        # 1c) DeQuantStub to convert final output back to FP32\n",
    "        self.dequant = tq.DeQuantStub()\n",
    "\n",
    "    def process(self, images):\n",
    "        return self.fp32_model.process(images)\n",
    "\n",
    "    def forward(self, images):\n",
    "        print(\"[QuantDinoFPN] Forwarding through the quantized model...\")\n",
    "        # 1) Quantize input: attaches observers to measure activation ranges\n",
    "        x = self.quant(images)\n",
    "        print(f\"[QuantDinoFPN] Input quantized, shape: {x.shape}, dtype: {x.dtype}\")\n",
    "\n",
    "        # 2) Forward through original FP32 model\n",
    "        logits = self.fp32_model(x)\n",
    "        print(f\"[QuantDinoFPN] Forwarded through FP32 model, logits shape: {logits.shape}, dtype: {logits.dtype}\")\n",
    "\n",
    "        # 3) Dequantize output (brings quantized int8 result back to float)\n",
    "        out = self.dequant(logits)\n",
    "        print(f\"[QuantDinoFPN] Output dequantized, shape: {out.shape}, dtype: {out.dtype}\")\n",
    "        return out\n",
    "\n",
    "    def fuse_model(self):\n",
    "        \"\"\"\n",
    "        Fuse Conv2d + BatchNorm2d + ReLU sequences, wherever they appear, across the entire network.\n",
    "        Because your FPNHead modules use exactly that pattern, this will fuse:\n",
    "            - Each proj: Conv2d → BatchNorm2d → ReLU → Dropout2d (Dropout is skipped in fusion)\n",
    "            - The fuse block: Conv2d → BatchNorm2d → ReLU → Dropout2d\n",
    "            - The classifier: Conv2d → BatchNorm2d → ReLU → Dropout2d → Conv2d  (we can fuse up to ReLU)\n",
    "        PyTorch automatically handles fusing only the Conv-BN-ReLU parts, leaving dropout alone.\n",
    "        \"\"\"\n",
    "        # Fuse in the head\n",
    "        head = self.fp32_model.head\n",
    "\n",
    "        # 1) Fuse each 1×1 proj: Conv → BatchNorm → ReLU\n",
    "        for idx, proj in enumerate(head.projs):\n",
    "            # proj is nn.Sequential([Conv2d, BatchNorm2d, ReLU, Dropout2d])\n",
    "            torch.quantization.fuse_modules(proj,\n",
    "                                           [\"0\", \"1\", \"2\"],  # fuse conv (idx 0), bn (idx 1), relu (idx 2)\n",
    "                                           inplace=True)\n",
    "\n",
    "        # 2) Fuse the fuse-block: Conv → BatchNorm → ReLU\n",
    "        torch.quantization.fuse_modules(head.fuse,\n",
    "                                       [\"0\", \"1\", \"2\"],  # indices: 0=Conv2d, 1=BatchNorm2d, 2=ReLU\n",
    "                                       inplace=True)\n",
    "\n",
    "        # 3) Fuse the first part of classifier: Conv → BatchNorm → ReLU\n",
    "        #    classifier = nn.Sequential([Conv2d, BatchNorm2d, ReLU, Dropout2d, Conv2d])\n",
    "        torch.quantization.fuse_modules(head.classifier,\n",
    "                                       [\"0\", \"1\", \"2\"],  # fuse conv(0), bn(1), relu(2)\n",
    "                                       inplace=True)\n",
    "        # The final Conv2d (index 4) cannot be fused further, since there's no BatchNorm or ReLU after.\n",
    "\n",
    "        print(\"[QuantDinoFPN] Fused all Conv2d+BatchNorm2d+ReLU sequences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3047b0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QuantPTQ] Created QuantDinoFPN.\n"
     ]
    }
   ],
   "source": [
    "# 4b) Instantiate QuantDinoFPN\n",
    "quant_model = QuantDinoFPN(num_labels=cfg.dataset.num_classes, model_cfg=cfg.model)\n",
    "quant_model.to(torch.device(\"cpu\"))\n",
    "print(\"[QuantPTQ] Created QuantDinoFPN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc2b54e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantDinoFPN(\n",
       "  (quant): QuantStub()\n",
       "  (fp32_model): DinoFPN(\n",
       "    (backbone): Dinov2Model(\n",
       "      (embeddings): Dinov2Embeddings(\n",
       "        (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "          (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): Dinov2Encoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x Dinov2Layer(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attention): Dinov2Attention(\n",
       "              (attention): Dinov2SelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (output): Dinov2SelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (layer_scale1): Dinov2LayerScale()\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Dinov2MLP(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_scale2): Dinov2LayerScale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (head): FPNHead(\n",
       "      (projs): ModuleList(\n",
       "        (0-3): 4 x Sequential(\n",
       "          (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Dropout2d(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (fuse): Sequential(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Dropout2d(p=0.1, inplace=False)\n",
       "      )\n",
       "      (classifier): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Dropout2d(p=0.1, inplace=False)\n",
       "        (4): Conv2d(256, 33, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b76ad530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_dtype(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name} is loaded in {param.dtype}\")\n",
    "\n",
    "def print_all_modules_after_convert(model):\n",
    "    \"\"\"See what modules exist after convert\"\"\"\n",
    "    print(\"\\n=== All Modules After Convert ===\")\n",
    "    for name, module in model.named_modules():\n",
    "        print(f\"{name:60s} → {type(module).__name__}\")\n",
    "\n",
    "def check_quantized_weights(model):\n",
    "    \"\"\"Look for quantized weight storage\"\"\"\n",
    "    print(\"\\n=== Checking for Quantized Weights ===\")\n",
    "    \n",
    "    quantized_count = 0\n",
    "    for name, module in model.named_modules():\n",
    "        module_type = type(module).__name__\n",
    "        \n",
    "        # Check for packed weights (quantized modules)\n",
    "        if hasattr(module, '_packed_params'):\n",
    "            print(f\"{name:50s} → QUANTIZED (has _packed_params) - {module_type}\")\n",
    "            quantized_count += 1\n",
    "        \n",
    "        # Check for other quantized indicators\n",
    "        elif any(x in module_type for x in ['Quantized', 'Packed', 'Int8']):\n",
    "            print(f\"{name:50s} → QUANTIZED MODULE: {module_type}\")\n",
    "            quantized_count += 1\n",
    "        \n",
    "        # Regular FP32 modules with weights\n",
    "        elif hasattr(module, 'weight') and hasattr(module.weight, 'dtype'):\n",
    "            print(f\"{name:50s} → FP32 weight: {module.weight.dtype}\")\n",
    "    \n",
    "    print(f\"\\nTotal quantized modules found: {quantized_count}\")\n",
    "\n",
    "def print_quantizable_modules(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Walk through model.named_modules() and print every submodule\n",
    "    whose qconfig is not None (i.e. it will be quantized).\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Modules with qconfig (will be quantized) ===\")\n",
    "    for name, module in model.named_modules():\n",
    "        if getattr(module, \"qconfig\", None) is not None:\n",
    "            print(f\"{name:50s} → {type(module).__name__}\")\n",
    "        else:\n",
    "            print(f\"{name:50s} → None\")\n",
    "\n",
    "def print_quantized_modules(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Walk through model.named_modules() and print any module whose\n",
    "    type indicates it was replaced by a quantized implementation.\n",
    "    \"\"\"\n",
    "    quantized_prefixes = (\n",
    "        \"QuantizedConv2d\",\n",
    "        \"Conv2dPackedParams\",   # older PyTorch versions\n",
    "        \"LinearPackedParams\",   # older PyTorch versions\n",
    "        \"QuantizedLinear\",\n",
    "        \"QuantizedReLU\",\n",
    "        \"Quantize\",\n",
    "        \"DeQuantize\",\n",
    "        \"FloatFunctional\",      # e.g. for quantized add/mul\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Quantized submodules ===\")\n",
    "    for name, module in model.named_modules():\n",
    "        cls_name = type(module).__name__\n",
    "        for prefix in quantized_prefixes:\n",
    "            if cls_name.startswith(prefix):\n",
    "                print(f\"{name:50s} → {cls_name}\")\n",
    "                break\n",
    "\n",
    "def print_observers(model: torch.nn.Module):\n",
    "    print(\"\\n=== Weight observers in the model ===\")\n",
    "    for name, module in model.named_modules():\n",
    "        if getattr(module, \"weight_fake_quant\", None) is not None:\n",
    "            breakpoint()\n",
    "            obs = module.weight_fake_quant\n",
    "            print(\n",
    "                f\"{name:60s} → weight observer={type(obs).__name__}, \"\n",
    "                f\"qscheme={obs.qscheme}\"\n",
    "            )\n",
    "    print(\"\\n=== Activation observers in the model ===\")\n",
    "    for name, module in model.named_modules():\n",
    "        if getattr(module, \"activation_post_process\", None) is not None:\n",
    "            obs = module.activation_post_process\n",
    "            print(\n",
    "                f\"{name:60s} → activation observer={type(obs).__name__}, \"\n",
    "                f\"qscheme={obs.qscheme}\"\n",
    "            )\n",
    "\n",
    "def inspect_weight_observers(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    After `prepare(model, inplace=True)`, each quantizable module—\n",
    "    whether a fused ConvReLU2d, an unfused Conv2d, or a Linear—will have\n",
    "    either `weight_fake_quant` or `weight_post_process` pointing at an observer.\n",
    "    This helper finds and prints them, without requiring you to guess the exact class.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Inspecting weight observers (universal search) ===\")\n",
    "    any_found = False\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        # Try both possible observer attributes (PyTorch version may vary)\n",
    "        obs = getattr(module, \"weight_fake_quant\", None)\n",
    "        if obs is None:\n",
    "            obs = getattr(module, \"weight_post_process\", None)\n",
    "\n",
    "        if obs is not None:\n",
    "            any_found = True\n",
    "            # We found a weight observer here. Print module name, observer type, and qscheme.\n",
    "            qscheme = obs.qscheme if hasattr(obs, \"qscheme\") else \"(no qscheme)\"\n",
    "            print(f\"{name:60s} → observer = {type(obs).__name__},  qscheme = {qscheme}\")\n",
    "\n",
    "            # If it has per‐channel min/max arrays, show their shape\n",
    "            if hasattr(obs, \"min_vals\") and hasattr(obs, \"max_vals\"):\n",
    "                print(f\"    - per‐channel min_vals shape = {tuple(obs.min_vals.shape)}, \"\n",
    "                      f\"max_vals shape = {tuple(obs.max_vals.shape)}\")\n",
    "            elif hasattr(obs, \"min_val\") and hasattr(obs, \"max_val\"):\n",
    "                print(f\"    - min_val = {obs.min_val:.4f}, max_val = {obs.max_val:.4f}\")\n",
    "\n",
    "            # If it's a HistogramObserver, you can peek at its histogram (optional)\n",
    "            if hasattr(obs, \"histogram\") and hasattr(obs, \"edges\"):\n",
    "                print(f\"    - histogram bins = {tuple(obs.histogram.shape)}, \"\n",
    "                      f\"edges = {tuple(obs.edges.shape)}\")\n",
    "\n",
    "    if not any_found:\n",
    "        print(\">>> No weight observers found. Are you sure you called `prepare(...)` on this exact model?\")\n",
    "\n",
    "def debug_quantization_setup(model: torch.nn.Module):\n",
    "    \"\"\"Debug what's happening with quantization setup\"\"\"\n",
    "    print(\"\\n=== Debugging Quantization Setup ===\")\n",
    "    \n",
    "    # Check if model has any qconfig attributes\n",
    "    has_qconfig = hasattr(model, 'qconfig') and model.qconfig is not None\n",
    "    print(f\"Model has qconfig: {has_qconfig}\")\n",
    "    if has_qconfig:\n",
    "        print(f\"Model qconfig: {model.qconfig}\")\n",
    "    \n",
    "    # Check all modules and their attributes\n",
    "    quantizable_modules = []\n",
    "    prepared_modules = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        # Check for qconfig\n",
    "        module_qconfig = getattr(module, 'qconfig', None)\n",
    "        \n",
    "        # Check for observer attributes\n",
    "        has_weight_obs = (hasattr(module, 'weight_fake_quant') or \n",
    "                         hasattr(module, 'weight_post_process'))\n",
    "        has_activation_obs = (hasattr(module, 'activation_post_process') or\n",
    "                            hasattr(module, 'activation_fake_quant'))\n",
    "        \n",
    "        # Check if it's a standard quantizable type\n",
    "        is_quantizable_type = isinstance(module, (\n",
    "            torch.nn.Conv1d, torch.nn.Conv2d, torch.nn.Conv3d,\n",
    "            torch.nn.Linear, torch.nn.ConvTranspose1d, torch.nn.ConvTranspose2d\n",
    "        ))\n",
    "        \n",
    "        if is_quantizable_type:\n",
    "            quantizable_modules.append(name)\n",
    "            \n",
    "        if has_weight_obs or has_activation_obs or module_qconfig:\n",
    "            prepared_modules.append((name, type(module).__name__, \n",
    "                                   has_weight_obs, has_activation_obs, module_qconfig))\n",
    "    \n",
    "    print(f\"\\nFound {len(quantizable_modules)} quantizable modules:\")\n",
    "    for name in quantizable_modules[:10]:  # Show first 10\n",
    "        print(f\"  - {name}\")\n",
    "    if len(quantizable_modules) > 10:\n",
    "        print(f\"  ... and {len(quantizable_modules) - 10} more\")\n",
    "    \n",
    "    print(f\"\\nFound {len(prepared_modules)} prepared modules:\")\n",
    "    for name, mod_type, has_w, has_a, qconf in prepared_modules:\n",
    "        print(f\"  - {name:40s} ({mod_type:15s}) weight_obs={has_w}, act_obs={has_a}, qconfig={qconf is not None}\")\n",
    "\n",
    "    return len(prepared_modules) > 0\n",
    "\n",
    "def check_fused_modules_observers(model):\n",
    "    \"\"\"Check if fused modules have the right observers\"\"\"\n",
    "    print(\"\\n=== Checking Fused Modules ===\")\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if 'ConvReLU2d' in str(type(module)):\n",
    "            print(f\"{name:50s} → {type(module).__name__}\")\n",
    "            \n",
    "            # Check if it has weight observer\n",
    "            weight_obs = getattr(module, 'weight_fake_quant', None)\n",
    "            if weight_obs is None:\n",
    "                weight_obs = getattr(module, 'weight_post_process', None)\n",
    "            \n",
    "            print(f\"  - Has weight observer: {weight_obs is not None}\")\n",
    "            if weight_obs:\n",
    "                print(f\"  - Weight observer type: {type(weight_obs).__name__}\")\n",
    "            \n",
    "            # Check qconfig\n",
    "            qconfig = getattr(module, 'qconfig', None)\n",
    "            print(f\"  - Has qconfig: {qconfig is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e8e917f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QuantPTQ] Validation dataset size: 783\n",
      "[QuantPTQ] Calibration dataset size: 35\n"
     ]
    }
   ],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 4) Main script: load FP32 checkpoint → wrap in QuantDinoFPN → fuse → set qconfig\n",
    "#    → prepare → calibrate → convert → evaluate → save INT8 weights\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 4a) Build a validation loader (no augmentations, just center-crop)\n",
    "crop_h, crop_w = (cfg.augmentation.crop_height, cfg.augmentation.crop_width)\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.CenterCrop(crop_h, crop_w)\n",
    "])\n",
    "\n",
    "val_dataset = KittiSemSegDataset(\n",
    "    root_dir='/home/panos/Documents/data/kitti-360',\n",
    "    train=False,\n",
    "    transform=val_transform\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=cfg.train.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.dataset.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "print(f\"[QuantPTQ] Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "cal_dataset = KittiSemSegDataset(\n",
    "    root_dir='/home/panos/Documents/data/kitti-360',\n",
    "    train=True,\n",
    "    calibration=True,\n",
    "    transform=val_transform\n",
    ")\n",
    "cal_loader = DataLoader(\n",
    "    cal_dataset,\n",
    "    batch_size=cfg.train.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=cfg.dataset.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "print(f\"[QuantPTQ] Calibration dataset size: {len(cal_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a425b9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QuantPTQ] Loaded FP32 weights from /home/panos/dev/hf_seg/checkpoints/dino-fpn-quantized.pth into fp32_model.\n"
     ]
    }
   ],
   "source": [
    "# 4c) Load your best FP32 checkpoint into `quant_model.fp32_model`\n",
    "ckpt_path = os.path.join(project_dir, \"checkpoints\", f\"{cfg.checkpoint.model_name}.pth\")\n",
    "if not os.path.exists(ckpt_path):\n",
    "    print(f\"[Error] Checkpoint not found: {ckpt_path}\")\n",
    "checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "quant_model.fp32_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "print(f\"[QuantPTQ] Loaded FP32 weights from {ckpt_path} into fp32_model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7c0aa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QuantDinoFPN] Fused all Conv2d+BatchNorm2d+ReLU sequences.\n"
     ]
    }
   ],
   "source": [
    "# 4d) Fuse Conv2d + BatchNorm2d + ReLU sequences in the head\n",
    "quant_model.eval()\n",
    "quant_model.fuse_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0208bde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, quant_min=0, quant_max=127){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "[QuantPTQ] Assigned QConfig.\n"
     ]
    }
   ],
   "source": [
    "# 4e) Assign QConfig: per‐channel weight quant + HistogramObserver (KL) activations\n",
    "#     - get_default_qconfig(\"fbgemm\") already uses PerChannelMinMaxObserver for weights\n",
    "#     - We swap the activation observer to HistogramObserver (KL)\n",
    "# - Build a per-channel symmetric observer for weights\n",
    "# weight_obs  = PerChannelMinMaxObserver.with_args(\n",
    "#     dtype=torch.qint8,\n",
    "#     qscheme=torch.per_channel_symmetric,\n",
    "#     ch_axis=0\n",
    "# )\n",
    "\n",
    "# # - Build a histogram (KL) observer for activations\n",
    "# activation_obs = HistogramObserver.with_args(\n",
    "#     dtype=torch.quint8,\n",
    "#     qscheme=torch.per_tensor_affine,\n",
    "#     reduce_range=False\n",
    "# )\n",
    "\n",
    "# # - Compose them into a QConfig\n",
    "# custom_qconfig = QConfig(\n",
    "#     activation=activation_obs,\n",
    "#     weight=weight_obs\n",
    "# )\n",
    "# quant_model.qconfig = custom_qconfig\n",
    "# quant_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "quant_model.qconfig = torch.quantization.default_qconfig\n",
    "print(quant_model.qconfig)\n",
    "print(\"[QuantPTQ] Assigned QConfig.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0968840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude_problematic_modules(model)\n",
    "# Exclude embeddings\n",
    "embeddings = quant_model.fp32_model.backbone.embeddings\n",
    "embeddings.qconfig = None\n",
    "for name, module in embeddings.named_modules():\n",
    "    module.qconfig = None\n",
    "print(\"Excluded Dinov2Embeddings from quantization\")\n",
    "\n",
    "# Exclude LayerNorm modules\n",
    "layernorm_count = 0\n",
    "for name, module in quant_model.named_modules():\n",
    "    if isinstance(module, torch.nn.LayerNorm):\n",
    "        module.qconfig = None\n",
    "        layernorm_count += 1\n",
    "        print(f\"Excluded LayerNorm: {name}\")\n",
    "\n",
    "print(f\"Excluded {layernorm_count} LayerNorm modules from quantization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90f122cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantDinoFPN(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (fp32_model): DinoFPN(\n",
       "    (backbone): Dinov2Model(\n",
       "      (embeddings): Dinov2Embeddings(\n",
       "        (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "          (projection): Conv2d(\n",
       "            3, 768, kernel_size=(14, 14), stride=(14, 14)\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): Dinov2Encoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x Dinov2Layer(\n",
       "            (norm1): LayerNorm(\n",
       "              (768,), eps=1e-06, elementwise_affine=True\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (attention): Dinov2Attention(\n",
       "              (attention): Dinov2SelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "                )\n",
       "              )\n",
       "              (output): Dinov2SelfOutput(\n",
       "                (dense): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (layer_scale1): Dinov2LayerScale()\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm(\n",
       "              (768,), eps=1e-06, elementwise_affine=True\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (mlp): Dinov2MLP(\n",
       "              (fc1): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "              (activation): GELUActivation()\n",
       "              (fc2): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "            (layer_scale2): Dinov2LayerScale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm(\n",
       "        (768,), eps=1e-06, elementwise_affine=True\n",
       "        (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "    (head): FPNHead(\n",
       "      (projs): ModuleList(\n",
       "        (0-3): 4 x Sequential(\n",
       "          (0): ConvReLU2d(\n",
       "            (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "          (3): Dropout2d(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (fuse): Sequential(\n",
       "        (0): ConvReLU2d(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (1): Identity()\n",
       "        (2): Identity()\n",
       "        (3): Dropout2d(p=0.1, inplace=False)\n",
       "      )\n",
       "      (classifier): Sequential(\n",
       "        (0): ConvReLU2d(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (1): Identity()\n",
       "        (2): Identity()\n",
       "        (3): Dropout2d(p=0.1, inplace=False)\n",
       "        (4): Conv2d(\n",
       "          256, 33, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4f) Prepare for static quantization (inserts observers)\n",
    "dev = next(quant_model.parameters()).device\n",
    "assert dev.type == \"cpu\"\n",
    "tq.prepare(quant_model, inplace=True)\n",
    "quant_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46de5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_param_dtype(quant_model)\n",
    "# check_fused_modules_observers(quant_model)\n",
    "# inspect_weight_observers(quant_model)\n",
    "# debug_quantization_setup(quant_model)\n",
    "# print_quantizable_modules(quant_model)\n",
    "# print_observers(quant_model)\n",
    "# print(\"[QuantPTQ] Model prepared for static quant (observers inserted).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92b6d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4g) Calibrate on ~500 validation batches\n",
    "# quant_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     cal_bar = tqdm(cal_loader, desc=\"Calibrating\")\n",
    "#     for batch_idx, (imgs, _) in enumerate(cal_bar, start=1):\n",
    "#         imgs = imgs.permute(0, 3, 1, 2).to(device) # [B, C, H, W]\n",
    "#         input = quant_model.fp32_model.process(imgs)\n",
    "#         _ = quant_model(input)  # Forward pass only: observers record histograms\n",
    "# print(f\"[Calibrate] Completed {batch_idx} batches of calibration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd574abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/panos/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/ao/quantization/utils.py:408: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantDinoFPN(\n",
       "  (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (fp32_model): DinoFPN(\n",
       "    (backbone): Dinov2Model(\n",
       "      (embeddings): Dinov2Embeddings(\n",
       "        (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "          (projection): QuantizedConv2d(3, 768, kernel_size=(14, 14), stride=(14, 14), scale=1.0, zero_point=0)\n",
       "        )\n",
       "        (dropout): QuantizedDropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): Dinov2Encoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x Dinov2Layer(\n",
       "            (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attention): Dinov2Attention(\n",
       "              (attention): Dinov2SelfAttention(\n",
       "                (query): QuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "                (key): QuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "                (value): QuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "              )\n",
       "              (output): Dinov2SelfOutput(\n",
       "                (dense): QuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "                (dropout): QuantizedDropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (layer_scale1): Dinov2LayerScale()\n",
       "            (drop_path): Identity()\n",
       "            (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Dinov2MLP(\n",
       "              (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "              (activation): GELUActivation()\n",
       "              (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "            )\n",
       "            (layer_scale2): Dinov2LayerScale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (head): FPNHead(\n",
       "      (projs): ModuleList(\n",
       "        (0-3): 4 x Sequential(\n",
       "          (0): QuantizedConvReLU2d(768, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "          (3): Dropout2d(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (fuse): Sequential(\n",
       "        (0): QuantizedConvReLU2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
       "        (1): Identity()\n",
       "        (2): Identity()\n",
       "        (3): Dropout2d(p=0.1, inplace=False)\n",
       "      )\n",
       "      (classifier): Sequential(\n",
       "        (0): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
       "        (1): Identity()\n",
       "        (2): Identity()\n",
       "        (3): Dropout2d(p=0.1, inplace=False)\n",
       "        (4): QuantizedConv2d(256, 33, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4h) Convert to INT8 (replace FP32 modules with quantized kernels)\n",
    "quant_model.eval()\n",
    "tq.convert(quant_model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bed7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_all_modules_after_convert(quant_model)\n",
    "# check_quantized_weights(quant_model)\n",
    "# print_param_dtype(quant_model)\n",
    "# print(\"[QuantPTQ] Model converted to INT8 (quantized kernels).\")\n",
    "# print_quantized_modules(quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cb32885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Eval]:   0%|          | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input type: torch.float32, shape: torch.Size([12, 3, 364, 1232])\n",
      "[QuantDinoFPN] Forwarding through the quantized model...\n",
      "[QuantDinoFPN] Input quantized, shape: torch.Size([12, 3, 364, 1232]), dtype: torch.quint8\n",
      "Quantized model detected - skipping dtype conversion\n",
      "Embeddings type: torch.quint8\n",
      "Embeddings dequantized type: torch.float32\n",
      "Embeddings type: torch.float32\n",
      "CLS tokens type: torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Eval]:   0%|          | 0/66 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'quantized::layer_norm' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::layer_norm' is only available for these backends: [Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMTIA, AutogradMeta, Tracer, AutocastCPU, AutocastMTIA, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at /pytorch/aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at /pytorch/aten/src/ATen/native/quantized/cpu/qnormalization.cpp:132 [kernel]\nBackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:194 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:479 [backend fallback]\nFunctionalize: registered at /pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:100 [backend fallback]\nAutogradOther: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradCPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradCUDA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:75 [backend fallback]\nAutogradXLA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:83 [backend fallback]\nAutogradMPS: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:91 [backend fallback]\nAutogradXPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:71 [backend fallback]\nAutogradHPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:104 [backend fallback]\nAutogradLazy: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:87 [backend fallback]\nAutogradMTIA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:79 [backend fallback]\nAutogradMeta: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:95 [backend fallback]\nTracer: registered at /pytorch/torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:322 [backend fallback]\nAutocastMTIA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocastXPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:504 [backend fallback]\nAutocastMPS: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:208 [backend fallback]\nPythonTLSSnapshot: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:475 [backend fallback]\nPreDispatch: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:198 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m quant_model\u001b[38;5;241m.\u001b[39mprocess(imgs)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mquant_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m           \u001b[38;5;66;03m# [B, num_classes, H, W]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m preds  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [B, H, W]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m miou_metric\u001b[38;5;241m.\u001b[39mupdate(preds, masks)\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[7], line 41\u001b[0m, in \u001b[0;36mQuantDinoFPN.forward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[QuantDinoFPN] Input quantized, shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# 2) Forward through original FP32 model\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp32_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[QuantDinoFPN] Forwarded through FP32 model, logits shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# 3) Dequantize output (brings quantized int8 result back to float)\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/dev/hf_seg/models/DinoFPNbn.py:128\u001b[0m, in \u001b[0;36mDinoFPN.forward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m    125\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# backbone → [B, 1+N, C] and attention maps\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m hstates \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mhidden_states \u001b[38;5;66;03m# [13] [B, 1+N, D]\u001b[39;00m\n\u001b[1;32m    130\u001b[0m taps \u001b[38;5;241m=\u001b[39m [hstates[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idxs]\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/transformers/models/dinov2/modeling_dinov2.py:705\u001b[0m, in \u001b[0;36mDinov2Model.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    701\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    703\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos)\n\u001b[0;32m--> 705\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    713\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(sequence_output)\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/transformers/models/dinov2/modeling_dinov2.py:517\u001b[0m, in \u001b[0;36mDinov2Encoder.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    510\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    511\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    512\u001b[0m         hidden_states,\n\u001b[1;32m    513\u001b[0m         layer_head_mask,\n\u001b[1;32m    514\u001b[0m         output_attentions,\n\u001b[1;32m    515\u001b[0m     )\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 517\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/transformers/models/dinov2/modeling_dinov2.py:459\u001b[0m, in \u001b[0;36mDinov2Layer.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    454\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    455\u001b[0m     head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    456\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    457\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m    458\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m--> 459\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# in Dinov2, layernorm is applied before self-attention\u001b[39;00m\n\u001b[1;32m    460\u001b[0m         head_mask,\n\u001b[1;32m    461\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    462\u001b[0m     )\n\u001b[1;32m    463\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    465\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_scale1(attention_output)\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/normalization.py:48\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_zero_point\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_point\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hf_seg/.venv/lib/python3.10/site-packages/torch/_ops.py:1158\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'quantized::layer_norm' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::layer_norm' is only available for these backends: [Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMTIA, AutogradMeta, Tracer, AutocastCPU, AutocastMTIA, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at /pytorch/aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at /pytorch/aten/src/ATen/native/quantized/cpu/qnormalization.cpp:132 [kernel]\nBackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:194 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:479 [backend fallback]\nFunctionalize: registered at /pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:100 [backend fallback]\nAutogradOther: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradCPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradCUDA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:75 [backend fallback]\nAutogradXLA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:83 [backend fallback]\nAutogradMPS: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:91 [backend fallback]\nAutogradXPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:71 [backend fallback]\nAutogradHPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:104 [backend fallback]\nAutogradLazy: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:87 [backend fallback]\nAutogradMTIA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:79 [backend fallback]\nAutogradMeta: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:95 [backend fallback]\nTracer: registered at /pytorch/torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:322 [backend fallback]\nAutocastMTIA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocastXPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:504 [backend fallback]\nAutocastMPS: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:208 [backend fallback]\nPythonTLSSnapshot: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:475 [backend fallback]\nPreDispatch: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:198 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "# 4i) Evaluate mIoU of the INT8 model\n",
    "with torch.no_grad():\n",
    "    quant_model.eval()\n",
    "    miou_metric = JaccardIndex(\n",
    "        task=\"multiclass\",\n",
    "        num_classes=NUM_CLASSES,\n",
    "        average=\"micro\",\n",
    "        ignore_index=255,\n",
    "    )\n",
    "\n",
    "    for imgs, masks in tqdm(val_loader, desc=\"[Eval]\"):\n",
    "        imgs  = imgs.permute(0, 3, 1, 2).float()\n",
    "        input = quant_model.process(imgs)\n",
    "        print(f\"Input type: {input.dtype}, shape: {input.shape}\")\n",
    "        logits = quant_model(input)           # [B, num_classes, H, W]\n",
    "        preds  = torch.argmax(logits, dim=1)  # [B, H, W]\n",
    "        miou_metric.update(preds, masks)\n",
    "\n",
    "    int8_miou = miou_metric.compute().item()\n",
    "print(f\"[QuantPTQ] INT8‐quantized model mIoU = {int8_miou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5dd283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4j) Save the INT8 state_dict\n",
    "save_path = os.path.join(project_dir, \"checkpoints\", \"dinofpn_int8.pth\")\n",
    "torch.save(quant_model.state_dict(), save_path)\n",
    "print(f\"[QuantPTQ] Saved INT8 weights to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
